# Cleansing data code source: https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0dimport pandas as pdimport numpy as npimport datacompyimport seaborn as snsimport matplotlib.pyplot as pltimport sysimport osimport matplotlib.cm as cmimport warningswarnings.filterwarnings('ignore')# Delete old excel fileos.remove("/33_Sales_Target_Region_Vehicle_Seats/4_Cleansing/Parc_2024.xlsx")###################################################################################################################################print('Step 1')from sqlalchemy import create_engineimport pandas as pdimport numpy as npimport sqlalchemy as safrom sqlalchemy.engine import URLconnection_string = "DRIVER={ODBC Driver 17 for SQL Server};SERVER=GPS-SRV20;DATABASE=Parc;UID=SMMT\\alvesd;PWD=;Trusted_connection=yes"connection_url = URL.create("mssql+pyodbc", query={"odbc_connect": connection_string})engine = create_engine(connection_url)# with engine.begin() as conn:#    seats = pd.read_sql_query(sa.text('''SELECT * FROM [DataShop];'''), conn)with engine.begin() as conn:    parc = pd.read_sql_query(        sa.text('''SELECT TOP (10000) * FROM [DataShop];'''), conn)print(parc)parc.to_excel(    'C:\\Users\\alvesd\\OneDrive - smmt.co.uk\\Desktop\\Diego_work_folder\\python\\4_Cleansing\\Parc_2024.xlsx',    index=False)parc = pd.read_excel(    'C:\\Users\\alvesd\\OneDrive - smmt.co.uk\\Desktop\\Diego_work_folder\\python\\4_Cleansing\\Parc_2024.xlsx')#################################################################################################################################### Join multiple columns and create unique columnparc['Unique'] = parc['Vehicle Type'].astype(str) + '/' + parc['Sector'].astype(str) + '/' + parc['Make'].astype(    str) + '/' + parc["Count of Registrations"].astype(    str)  # + '/' + parc["Trans"].astype(str)# Join multiple columns and create unique column# Move the unique column to the first positionfirst_column = parc.pop('Unique')parc.insert(0, 'Unique', first_column)# import packagesimport pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport matplotlib.mlab as mlabimport matplotlibplt.style.use('ggplot')from matplotlib.pyplot import figure# %matplotlib inline#matplotlib.rcParams['figure.figsize'] = (12, 10)pd.options.mode.chained_assignment = None# read the data# df = pd.read_csv('sberbank.csv')# shape and data types of the dataprint(parc.shape)print(parc.dtypes)# select numeric columnsdf_numeric = parc.select_dtypes(include=[np.number])numeric_cols = df_numeric.columns.valuesprint(numeric_cols)# select non numeric columnsdf_non_numeric = parc.select_dtypes(exclude=[np.number])non_numeric_cols = df_non_numeric.columns.valuesprint(non_numeric_cols)cols = parc.columns[:30]  # first 30 columnscolours = ['#000099', '#ffff00']  # specify the colours - yellow is missing. blue is not missing.fig, ax = plt.subplots(figsize=(12, 10))sns.heatmap(parc[cols].isnull(), cmap=sns.color_palette(colours), ax=ax)plt.autoscale()plt.show()# if it's a larger dataset and the visualization takes too long can do this.# % of missing.for col in parc.columns:    pct_missing = np.mean(parc[col].isnull())    print('{} - {}%'.format(col, round(pct_missing * 100)))# first create missing indicator for features with missing datafor col in parc.columns:    missing = parc[col].isnull()    num_missing = np.sum(missing)    if num_missing > 0:        print('created missing indicator for: {}'.format(col))        parc['{}_ismissing'.format(col)] = missing# then based on the indicator, plot the histogram of missing valuesismissing_cols = [col for col in parc.columns if 'ismissing' in col]parc['num_missing'] = parc[ismissing_cols].sum(axis=1)# first_column = parc.pop('num_missing')# parc.insert(0, 'num_missing', first_column)print(parc.head())#parc['num_missing'].value_counts().reset_index().sort_values(by='index').plot.bar(x='index', y='num_missing')#parc['num_missing'].value_counts().reset_index().plot.bar(title='Num Missing')# creating the bar plot: # For example, there are over 7000 observations with no missing values and close to 3000 observations with one missing value.plt.bar(parc['num_missing'], parc['Count of Registrations'], color ='maroon',        width = 0.4)plt.xlabel("Count of at least missing values")plt.ylabel("No. of rows with missing values")plt.title("Missing values")plt.show()# parc['num_missing'].value_counts().reset_index().plot.bar(title='Num Missing')plt.show()# impute the missing values and create the missing value indicator variables for each non-numeric column.df_non_numeric = parc.select_dtypes(exclude=[np.number])non_numeric_cols = df_non_numeric.columns.valuesfor col in non_numeric_cols:    missing = parc[col].isnull()    num_missing = np.sum(missing)    if num_missing > 0:  # only do the imputation for the columns that have missing values.        print('imputing missing values for: {}'.format(col))        parc['{}_ismissing'.format(col)] = missing        top = parc[col].describe()['top']  # impute with the most frequent value.        parc[col] = parc[col].fillna(top)# categoricalparc['Aspiration'] = parc['Aspiration'].fillna('_MISSING_')# numericparc['Drive_Type'] = parc['Drive_Type'].fillna(-999)print(parc.head())# histogram of life_sq.parc['Count of Registrations'].hist(bins=100)plt.show()# box plot.parc.boxplot(column=['Count of Registrations'])plt.show()####################################################################################################################################Unnecessary type #1: Uninformative / Repetitive#Sometimes one feature is uninformative because it has too many rows being the same value.#How to find out?#We can create a list of features with a high percentage of the same value.#For example, we specify below to show features with over 95% rows being the same value.num_rows = len(parc.index)low_information_cols = []  #for col in parc.columns:    cnts = parc[col].value_counts(dropna=False)    top_pct = (cnts / num_rows).iloc[0]    if top_pct > 0.95:        low_information_cols.append(col)        print('{0}: {1:.5f}%'.format(col, top_pct * 100))        print(cnts)        print()# Inconsistent type #1: Capitalization# Inconsistent usage of upper and lower cases in categorical values is a common mistake. It could cause issues since analyses in Python is case sensitive.print(parc['Make'].value_counts(dropna=False))# Inconsistent type: Formats# Another standardization we need to perform is the data formats. One example is to convert the feature from string to DateTime format.# We can convert it and extract the date or time values by using the code below. After this, it’s easier to analyze the transaction volume group by either year or month.parc['Year of 1st Reg'] = pd.to_datetime(parc['Year of 1st Reg'], format='%Y')parc['Year of 1st Reg'] = parc['Year of 1st Reg'].dt.year#df['month'] = df['timestamp_dt'].dt.month#df['weekday'] = df['timestamp_dt'].dt.weekdayprint(parc['Year of 1st Reg'].value_counts(dropna=False))#print()#print(df['month'].value_counts(dropna=False))# Inconsistent type #3: Categorical Values# Inconsistent categorical values are the last inconsistent type we cover. A categorical feature has a limited number of values. Sometimes there may be other values due to reasons such as typos.#from nltk.metrics import edit_distance#parc['Country of Origin'] = parc['Country of Origin'].map(lambda x: edit_distance(x, 'United Kingdom'))#parc['Country of Origin'] = parc['Country of Origin'].map(lambda x: edit_distance(x, 'France'))# We can set criteria to convert these typos to the correct values. For example, the below code sets all the values within 2 letters distance from “United Kingdom” to be “United Kingdom”.#msk = parc['Country of Origin'] <= 2#parc.loc[msk, 'Country of Origin'] = 'United Kingdom'#msk = parc['Country of Origin'] <= 2#parc.loc[msk, 'Country of Origin'] = 'France'print(parc.head())################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################Create a TEXT file for printing the reportreport = open('/33_Sales_Target_Region_Vehicle_Seats/4_Cleansing/MP_Cleansing/Parc_Cleansing_Report.txt', 'w')parc.info()# The isna method returns a DataFrame of all boolean values (True/False).parc_null1 = parc.isna()parc_null = parc_null1.sum()report.write('\n' + '\n' + 'Nulls values by columns in the data: Parc' + str(parc_null.sum) + '\n' + '\n')## Missing Data Percentage Listfor col in parc.columns:    pct_missing = np.mean(parc[col].isnull())    percentage = '{} - {}%'.format(col, round(pct_missing * 100))    report.write('Missing Data by Percentage Parc: ' + str(percentage) + '\n')    # print(percentage)# Visualizing Missing Data using Seaborn heatmap()plt.figure(figsize=(10, 6))sns.heatmap(parc.isna().transpose(),            cmap="YlGnBu",            cbar_kws={'label': 'Missing Data'})plt.savefig("Parc visualizing_missing_data_with_heatmap_Seaborn_Pytthon.png", dpi=100)plt.show()# Visualizing Missing Data using Seaborn displot()plt.figure(figsize=(10, 6))sns.displot(    data=parc.isna().melt(value_name="missing").astype(str),    y="variable",    hue="missing",    multiple="fill",    aspect=1.25)plt.savefig("Parc visualizing_missing_data_with_barplot_Seaborn_distplot.png", dpi=100)plt.show()################################################################################Beginning of Part2# Load the data: Index of F:\AIS\Motorparc\Standard Reports\2021 Census\df2 = pd.read_excel('C:\\Users\\alvesd\\OneDrive - smmt.co.uk\\Desktop\\Diego_work_folder\\python\\4_Cleansing\\Parc_2024.xlsx')# Join multiple columns and create unique columndf2['Unique'] = df2['Vehicle Type'].astype(str) + '/' + df2['Sector'].astype(str) + '/' + df2['Make'].astype(    str) + '/' + df2["Count of Registrations"].astype(    str)  # + '/' + parc["Trans"].astype(str)# Join multiple columns and create unique column# Move the unique column to the first positionfirst_column = df2.pop('Unique')df2.insert(0, 'Unique', first_column)# The info method prints to the screen the number of non-missing values of each columndf2.info()# The ISNA method returns a DataFrame of all boolean values (True/False).df2_null1 = df2.isna()df2_null = df2_null1.sum()# Save the ISNA results to a TXT filereport.write('\n' + '\n' + 'Nulls values by columns in the data Parc Raw: ' + str(df2_null.sum) + '\n' + '\n')## Looping: Missing Data Percentage List and save to a TXT filefor col in df2.columns:    pct_missing = np.mean(df2[col].isnull())    percentage = '{} - {}%'.format(col, round(pct_missing * 100))    report.write('Missing Data by Percentage Parc Raw: ' + str(percentage) + '\n')    # print(percentage)# Visualizing Missing Data using Seaborn heatmap()plt.figure(figsize=(10, 6))sns.heatmap(df2.isna().transpose(),            cmap="YlGnBu",            cbar_kws={'label': 'Missing Data'})plt.savefig("Parc Raw visualizing_missing_data_with_heatmap_Seaborn_Pytthon.png", dpi=100)plt.show()# Visualizing Missing Data using Seaborn displot()plt.figure(figsize=(10, 6))sns.displot(    data=df2.isna().melt(value_name="missing").astype(str),    y="variable",    hue="missing",    multiple="fill",    aspect=1.25)plt.savefig("Parc Raw visualizing_missing_data_with_barplot_Seaborn_distplot.png", dpi=100)plt.show()#################################################################################End of Part2###########################################################################Beginning of Part3# Display the report. Variance results:compare = datacompy.Compare(    parc,    df2,    join_columns=['Unique'],  # You can also specify a list of columns    abs_tol=0.0001,    rel_tol=0,    df1_name='original',    df2_name='new')# Save the variance results to a TXT filereport.write('\n' + '\n' + 'Cross variance results:' + str(compare.report()))report.write('\n' + '\n' + 'Unique rows: parc' + str(compare.df1_unq_rows))report.write('\n' + '\n' + 'Unique rows: parc raw' + str(compare.df2_unq_rows))report.write('\n' + '\n' + 'Unique columns: parc' + '\n' + str(compare.df1_unq_columns()))report.write('\n' + '\n' + 'Unique columns: parc raw' + '\n' + str(compare.df2_unq_columns()))